{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost workshop (R version)\n",
    "by Qi XU & Maxime KINET\n",
    "\n",
    "The goal of this notebook is to present the use of XGBoost in a practical case and some of the functionalities which makes it so useful. XGBoost is an implementation of a Gradient Boosted decision tree algorithm and is frequently used in winning solutions of data science competitions. XGBoost is an open source library, implemented in C++ with wrappers available in Python and R (among others). We have created two versions of this notebook, one in R and one in Python.\n",
    "\n",
    "The dataset we have chosen to illustrate our purposes comes from the recent B2C competition - Predict conversion rates on breakdown services. The goal was to predict the likelihood of residential customers to subscribe to a maintenance contract. \n",
    "\n",
    "More information on this dataset is available [here](https://datascience-challenge.engie.com/#/challenge/28733).\n",
    "\n",
    "\n",
    "## Before we begin...\n",
    "\n",
    "If you want to be able to run the notebook, you need to have XGBoost installed on your machine. Installation information is available on the [doc page of XGBoost](https://xgboost.readthedocs.io/en/latest/build.html).\n",
    "\n",
    "#### R users\n",
    "\n",
    "If you are a R user, XGBoost is now available as a CRAN package hence a simple `install.packages('xgboost')` should get you up and running.\n",
    "\n",
    "Aside from XGBoost, the following libraries are expected to be installed :\n",
    "\n",
    "1. ggplot2 (for graphics)\n",
    "2. Ckmeans.1d.dp (for variable importance plots)\n",
    "\n",
    "\n",
    "#### Python users\n",
    "\n",
    "Python users can use pip to install xgboost *provided* that they are not using windows. On windows, installation can be more complicated and we refer to the documentation for complete instructions.\n",
    "\n",
    "Aside from xgboost, we expect users to have the following libraries installed :\n",
    "1. pandas (for data frame manipulation)\n",
    "2. numpy (for scientific computing)\n",
    "3. matplotlib (for visualization)\n",
    "\n",
    "\n",
    "## A few words on XGBoost\n",
    "\n",
    "A lot has been written on XGBoost since its wide adoption by the datascience community. If you want to learn more about it, here are some links where you can start :\n",
    "\n",
    "1. [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/)\n",
    "2. [XGBoost Github repository](https://github.com/dmlc/xgboost)\n",
    "3. [A presentation by Tianqi Chen](https://www.youtube.com/watch?v=zyjYlU7in3I)\n",
    "4. [A reference paper](http://dmlc.cs.washington.edu/data/pdf/XGBoostArxiv.pdf)\n",
    "\n",
    "\n",
    "## Import XGBoost and other packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install the package if you haven't done it before\n",
    "#install.packages(\"xgboost\", repos=\"http://cran.rstudio.com/\")\n",
    "#install.packages(\"Ckmeans.1d.dp\",repos=\"http://cran.rstudio.com/\")\n",
    "library(xgboost)\n",
    "library(Ckmeans.1d.dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test data sets\n",
    "\n",
    "We assume that the files `train.CSV` and `test.CSV` are located in the same directory as the notebook. If it is not the case, you can replace the variable `workdir` by the full path to your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workdir = \"./\"\n",
    "# Read the csv file and store the data in a dataframe\n",
    "train <- read.table(paste0(workdir,\"train.CSV\"),header=TRUE,sep=\",\",na.strings=c(\"NA\",\"NaN\", \"\", \" \"))\n",
    "cat('The Training set contains',nrow(train),'observations and',ncol(train),'variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test <- read.csv(paste0(workdir,\"test.CSV\"),na.strings=c(\"NA\",\"NaN\", \"\", \" \"))\n",
    "cat('The Test set contains',nrow(test),'observations and',ncol(test),'variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isolate target variable\n",
    "target <- train$souscrit\n",
    "# Define the target variable for the test set as NaN.\n",
    "test$souscrit <- NA\n",
    "# Bind Training and Test set. We do this to make sure that every operation is performed identically \n",
    "# on the training and on the test set\n",
    "db <- rbind(train,test)\n",
    "cat('The Data set contains',nrow(db),'observations and',ncol(db),'variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Before firing XGBoost, we need to prepare the dataset. For this simple notebook, we will perform two tasks :\n",
    "1. Handle Missing values\n",
    "2. Convert all non-numerical variables to numerical.\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "The data set contains many missing values on several variables. Let us count how many values are missing for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this will print a summary of Missing Values count for each variable\n",
    "sapply(db,function(x)sum(is.na(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different strategies to impute missing value and usually we would carefully look at the data and decide what is the best strategy for variables with missing values. However, for the sake of this exercice, we will use one of the nice feature of XGBoost which allow us to do ***nothing*** about missing value and leave them as they are.\n",
    "\n",
    "There are potentially 41 predictors, but we will not use all of them to facilitate the treatment in this tutorial. We will exclude for instance all the categorical variable and keep the numerical variables and the dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Names of the numerical variables\n",
    "num_var_name = c('AGE_TR_PRED_FMT','CAP_EC','CAR_CHAR','CEL_FMT','CRC','DARTY','FACT_LIGN_FMT', \n",
    "               'MAILING','MA_RLV_FMT','NB_COHABITANT','PAP', 'PARTENAIRES', 'PAYEUR_DIVERG','PREA', 'PUIS_SOUS', 'REVENU_IRIS',\n",
    "               'TV', 'ass_fact','nb_cont_COUR_ENT', 'nb_cont_MAIL_ENT', 'nb_cont_RECLAMATION',\n",
    "               'nb_cont_RECLAMATION_6M', 'nb_cont_TEL_ENT', 'nb_contact_entrant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentionned earlier, the dates need to be converted to numerical variables to be usable by XGBoost. For each date we can extract :\n",
    "* the month (number from 1 to 12)\n",
    "* the day of the week (number from 1 to 7)\n",
    "* the day of the year (number from 1 to 365)\n",
    "* the year\n",
    "\n",
    "For the variable `DT_DEBT_ASSR` we will extract the first three features, while for `DT_ANC_CLI` and `DT_EMMENAG` we will keep only the year which, intuitively is the most meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform of timestamps\n",
    "db$DATE_ANC_CLI_year <- floor(as.integer(as.Date('2017-01-01') -as.Date(db$DATE_ANC_CLI))/365)\n",
    "db$DATE_EMMENAG_year <- floor(as.integer(as.Date('2017-01-01') -as.Date(db$DATE_EMMENAG))/365)\n",
    "date_timestamp <- as.POSIXlt(db$DT_DEBT_ASSR)\n",
    "db$DT_DEBT_ASSR_month <- date_timestamp$mon\n",
    "db$DT_DEBT_ASSR_week_day <- date_timestamp$wday\n",
    "db$DT_DEBT_ASSR_yday <- as.integer(as.Date(db$DT_DEBT_ASSR)-as.Date('2015-01-01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the final data set from the numerical variables and the modified timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_final <- db[,c(num_var_name,c(\"DT_DEBT_ASSR_yday\",\"DT_DEBT_ASSR_month\",\"DT_DEBT_ASSR_week_day\",\"DATE_ANC_CLI_year\",\"DATE_EMMENAG_year\"))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seperate train & test\n",
    "train <- db_final[1:127472,]\n",
    "test <- db_final[127473:254945,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with XGBoost \n",
    "\n",
    "### Parameters definition\n",
    "\n",
    "XGBoost has a lot of parameters and we will not use nor present all of them. Please refer to the documentation of XGBoost for more information.\n",
    "Let us start with a minimal set of parameters to make a first attempt. \n",
    "1. General parameters:\n",
    "    * nthread : As XGBoost is multithreaded, we can define here the number of thread available.\n",
    "2. Learning Task parameter:\n",
    "    * objective : This defines the task at hand. In our case, we have a binary classification and we want to compute a probability so we need a logistic transformation of the score function computed by xgboost.\n",
    "    * eval_metric : This defines the metric on which the algorithm is evaluated. Here we choose `auc` (Area Under the Curve), but many other metrics are available (logloss, accuracy, mean average precision,...)\n",
    "3. Tree booster parameters:\n",
    "    * eta : Learning rate, governs the convergence speed of the algorithm. The smaller is usually the better, but is definitely slower.\n",
    "    * max_depth : maximum depth of the trees. We will start with stumps (`max_depth = 1`).\n",
    "    \n",
    "Finally, we need to set the number of successive trees (`num_trees`) that XGBoost will create to make the classification. We will start by a very small number to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define parameters we are going to use\n",
    "params_1 = list(\"nthread\" = 3,\n",
    "              \"objective\" = \"binary:logistic\",\n",
    "              \"eval_metric\" = \"auc\",\n",
    "              \"eta\" = 0.5,\n",
    "              \"max_depth\" =  1)\n",
    "num_trees = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model\n",
    "\n",
    "The algorithm will be trained on the training set. To evaluate the performance of the algorithm accurately, we have to use data that the algorithm has never seen, otherwise the model will be overfitted. \n",
    "\n",
    "We will randomly select a subset from the training set and use it as a validation set. That is, we will predict the target on the validation set, and compare it to the true class to obtain a rough estimate of what the auc will be on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation set consist of 20000 randomly selected clients\n",
    "set.seed(143)\n",
    "idx <- sample(nrow(train),20000)\n",
    "xvalid <- train[idx,]\n",
    "xtrain <- train[-idx,]\n",
    "target_valid <- target[idx]\n",
    "target_train <- target[-idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important step is to convert the training and validation set to a DMatrix format (which is a xgboost-specific format). We do that in the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain <- xgb.DMatrix(data=data.matrix(xtrain),label=target_train,missing=NA)\n",
    "dvalid <- xgb.DMatrix(data=data.matrix(xvalid),label=target_valid,missing=NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note : `None` is the default for missing parameter in python, but you can change it if you use another convention in your dataset. \n",
    "For instance, you can choose that all values equal to `-99999` correspond to a missing value and xgboost will treat it as missing.\n",
    "\n",
    "Now let us train the model. In Xgboost, we can pass as argument a *watchlist* which is a list of dataset for which we wish to compute the performance metric regularly. In the watch list, we put our validation set and our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watchlist <- list(valid=dvalid,train=dtrain)\n",
    "xgb_simple <- xgb.train(param = params_1, \n",
    "                        data = dtrain, \n",
    "                        nrounds = num_trees, \n",
    "                        watchlist = watchlist, \n",
    "                        print.every.n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It works and we can even expect a pretty decent score (auc ~ 0.76). Now let us see how we can improve the algorithm.\n",
    "\n",
    "### Second model\n",
    "\n",
    "First, we should improve our validation methodolgy. Watchlist are useful for fast evaluation, but they can be inacurate. We will make a proper cross-validation instead.\n",
    "\n",
    "Second there is the possibility in XGBoost to tell it to stop when the evaluation metric (on the validation data) does not improve for a certain number of iterations. This is very useful to evaluate the number of iterations that are needed to optimize performance. We will tell xgboost to step if the auc does not increase for 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rebuild the DMatrix with full training set.\n",
    "dtrain <- xgb.DMatrix(data=data.matrix(train),label=target,missing=NA)\n",
    "# Set the maximum number of trees to a high value\n",
    "num_trees <- 1000\n",
    "# Launch 5-fold cross- validation.\n",
    "set.seed(954)\n",
    "xgb_cv <- xgb.cv(params=params_1,\n",
    "                 data=dtrain,\n",
    "                 nrounds=num_trees,\n",
    "                 nfold=5,\n",
    "                 early.stop.round=10,\n",
    "                 maximize=TRUE,\n",
    "                 print.every.n=25)\n",
    "\n",
    "# find optimal number of iterations\n",
    "cat('Highest AUC is', max(xgb_cv[['test.auc.mean']],na.rm=TRUE),'obtained at iteration', which.max(xgb_cv[['test.auc.mean']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning\n",
    "\n",
    "Now we can play with some parameters of XGBoost. We can modify parameters to improve the accuracy of the booster:\n",
    "1. Reducing the learning rate : eta = 0.1\n",
    "2. Increasing the depth of the trees : we allow trees to grow a bit deeper (up to depth =5).\n",
    "\n",
    "Next, since the trees we grow are more complex, we might want to tune other parameters to avoid overfitting\n",
    "1. min_child_weight defines the minimum (weighted) number of observations in a child to split a node.\n",
    "2. subsample : to introduce randomization, we can use only a subset of the training set to grow the trees. We set it to .9 (i.e. each tree will be trained on 90% of the data).\n",
    "\n",
    "With these adjusted parameters we can retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define New set of parameters\n",
    "params_2 = list(\"nthread\" = 3,\n",
    "              \"objective\" = \"binary:logistic\",\n",
    "              \"eval_metric\" = \"auc\",\n",
    "              \"eta\" = 0.1,\n",
    "              \"max_depth\" = 4,\n",
    "              \"min_child_weight\"= 20,\n",
    "              \"subsample\"= 0.90)\n",
    "set.seed(954)\n",
    "# Launch 5-fold cross- validation.\n",
    "xgb_cv = xgb.cv(params=params_2,\n",
    "                data=dtrain,\n",
    "                nrounds=num_trees,\n",
    "                nfold=5,\n",
    "                early.stop.round=10,\n",
    "                maximize=TRUE,\n",
    "                print.every.n=25)\n",
    "# find optimal number of iterations\n",
    "cat('Highest AUC is', max(xgb_cv[['test.auc.mean']],na.rm=TRUE),'obtained at iteration', which.max(xgb_cv[['test.auc.mean']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, we have improved our score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance\n",
    "\n",
    "Another great feature of XGBoost is its ability to compute an importance score for the predictors. This allow to understand better on what ground the prediction are made. \n",
    "\n",
    "In the code snippet below, we first retrain the model on the full training set, with the optimal parameters found above, and then we plot the variables importance in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train xgboost on the full training set\n",
    "xgb_final <- xgb.train(params=params_2,\n",
    "                      data=dtrain,\n",
    "                      nrounds=160)\n",
    "# feature importance to see which features are more importants than others\n",
    "importanceRaw <- xgb.importance( model = xgb_final,feature_names = names(train))\n",
    "xgb.plot.importance(importanceRaw,numberOfClusters = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set prediction \n",
    "\n",
    "The last think that remain to do is to make a prediction on the test set, and to prepare the submission file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtest <- xgb.DMatrix(data=data.matrix(test),missing=NA)\n",
    "pred <- predict(xgb_final,dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission=read.csv(paste0(workdir,\"sample_submission.CSV\"))\n",
    "submission$predictions = pred\n",
    "#write.csv(submission,file=paste0(fold_ad,\"myfirst_submission.CSV\"),sep=\",\",dec=\".\",quote=FALSE,row.names = FALSE)\n",
    "write.csv(submission,file=\"myfirst_submission.CSV\",row.names = FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations !!\n",
    "\n",
    "You have completed your first xgboost workshop!\n",
    "\n",
    "Few notes :\n",
    "\n",
    "- XGBoost is also available in scikit-learn, which is definitely worth exploring\n",
    "- Things we have not covered : sparse matrix, other metrics, customised objectives,...\n",
    "- There are most certainly some XGBoost tricks that we don't know about!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
